---
title: "Beating the booky with xgBoost [1]"
author: "Tobias Diederich"
date: "18 März 2016"
output: html_document
---

Beating the booky in prediction performance is an ambitious goal. Odds calculating companies (further refered to as bookys) put a huge effort in calculating accurate probabilities. I expect them to have large statistic branches with a lot of manual predicting support from sport analysts. Never the less, there are conditions, which could be exploited to gain an advantage: 
1. Market behaviour: Bookys adjust their odds depending on market behaviour, which is expected to be irrational  
2. Feature variety: Using different kinds of features than bookies  
3. Domain limitation: Concentrating on German 1. Bundesliga  
  
  
This blog is meant to be a continous story. In this first post I build a model with features, solely engineered from marketprices of participating players. In future work these features will be supplemented by shortterm form values for players, engineered from grades given by the famous german sportsmagazin 'Kicker'. Additional preprocessing, dimension reduction and further modeling technics will be applied.  
To make a start, the modeling algorithm xgboost is used, because it is known to be high perfoming for different problems. Several winning solutions on kaggle used it.

## Goal

This work aims on implementing a model to predict accurate probabilities, which applied in a betting strategy against booky odds would simulate a preferably high percentage profit. This simulated percentage profit is implemented as a custom metric and integrated in carets model tuning process.


## Data

The match, team and player data are collected from [transfermarkt.de](http://www.transfermarkt.de/) and [kicker.de](http://www.kicker.de/). Booky odds are collected from [sfstats.net](http://de.sfstats.net/).

Relevant data will be of germanies 1. Bundesliga for seasons 2005-2015.  

```{r echo=FALSE, results='hide',message=FALSE}
library(ggplot2)
library(vioplot)
library(corrplot)
library(polycor)
library(RMySQL)
library(caret)
library(tidyr)
library(properties)
library(e1071)
library(pROC)
library(gridExtra)
library(magrittr)
library(MASS)
library(gbm)
library(bnclassify)
library(C50)
library(kernlab)
library(xgboost)
library(testthat)
library(Hmisc)
library(plyr)
library(dplyr)
```

```{r warning=FALSE, message=FALSE, cache=TRUE}
source(file = '../dataProvider/loadData.R', echo = FALSE, encoding = 'UTF-8')
toMatchday <- 34
seasons <- c('2005-2006', '2006-2007', '2007-2008', '2008-2009', '2009-2010', 
             '2010-2011', '2011-2012', '2012-2013', '2013-2014', '2014-2015')
leagues <- c('BL1')
data <- loadTrainingData(toMatchday, seasons, leagues, dbName = 'soccerlabdata2.0')
matches <- data$matches
odds <- data$odds
stats <- data$stats
```

* matches: Matches for Germanies 1. Bundesliga for seasons 2005-2015  
* odds: Booky odds and probabilies for all matches  
* stats: Player - match data, including played position, actual marketprice and kicker grade, the player got for his performance  

There are ´r nrow(matches)´matches and ´r nrow(stats)´ related player - match data.

Player marketprice statistics:

```{r warning=FALSE, message=FALSE}
summary(stats$fitPrice)
```


### Remarks on booky odds

The bookies probability of a match outcome is the odds reciproke. For a match their probabilities do not sum up to 1, but to a value above 1. This is a booky margin, the main cause they make money and a big disadvantage for the punter. This margin decreases the payout in comparison to a fair bet.    

Booky margin statistics:

```{r warning=FALSE, message=FALSE}
exploreOdds <- mutate(odds, bookyProbSum = 
                          HomeVictory + VisitorsVictory + Draw - 1)
filter(exploreOdds, bookyProbSum < 0)
summary(exploreOdds$bookyProbSum)
```

## Feature Engineering

The engineered features solely correspond to the marketprices of players. All players in the starting lineup are taken into consideration. The players on bench participate in specially dedicated features. The prices are grouped by Position (tw, def, mid, off), Team (Home, Visitors) and aggregation Function (min, max, avg, sum).

```{r warning=FALSE, message=FALSE, cache=TRUE}
source(file = '../featureEngineering/positionFeatureExtraction.R', echo = FALSE, encoding = 'UTF-8')
assignedPositions <- c('tw', 'def', 'def', 'def', 'mid', 'mid', 'mid', 
                       'mid', 'off', 'off', 'off', 'off', 'off')
relNormalAssignments <- c('DURCHGESPIELT', 'AUSGEWECHSELT')
priceFuncts <- c('min', 'max', 'avg', 'sum')
benchPriceFuncts <- c('max', 'avg')
priceFeaturedMatches <- extractMatchResultFeatures(stats, matches, assignedPositions, relNormalAssignments, priceFuncts = priceFuncts, benchPriceFuncts = benchPriceFuncts)

features <- priceFeaturedMatches[, grepl('Price', colnames(priceFeaturedMatches))]
```

Some features for bench players and for position tw (Torwart = goaly) are left out, because they don't add much value.

So we have `r length(features)` features. 

### Feature correlation

Let's explore the features in comparison to each other. For the sake of overview I select just the features for home teams.

```{r warning=FALSE, message=FALSE}
# Position and Function correlation for price features of home teams
priceHomeData <- features[, grepl('Home', colnames(features))]
colnames(priceHomeData) <- gsub('_Price', '', gsub('_Home', '', colnames(priceHomeData)))
priceCorrData <- cor(priceHomeData)
corrplot(priceCorrData, order = 'AOE',
         title = 'Position and Function correlations for price features of home teams')
```

TODO Interpretation & Evtl. delting some features


## Model fitting and tuning

The target of the model tuning process is to find a model which maximizes the simulated profit [%]. This is a custom metric and implemented in the function betMetricsSummary. This function is integrated in the caret resampling and tuning process (caret is so great!!). I did some prework to find areas of parameter values which tend to give good results. Here, just the fine tuning is done.  
  
The applied model is extreme gradient boosting, a popular algorithm which is known to have very good and stable predictive performance in various different cases. Additionally it has a lot of fine tuning potential and supports parallel processing.


### Configuration of the fitting process

I use 5-fold cross validation first. 10-fold would be probably better, but calculation would take much longer.

```{r warning=FALSE, message=FALSE}
source(file = '../models/models.R', 
       echo = FALSE, encoding = 'UTF-8')
modelInput <- selectModelInput(priceFeaturedMatches)
seed <- 16450
customCvContr <- trainControl(method = 'cv', number = 5, classProbs = TRUE, 
                              summaryFunction = betMetricsSummary)

resultFormula <- as.formula('matchResult ~ . -matchId -goalsHome -goalsVisitors -goalDiff')
```

### Extreme Gradiant Boosing

Tuning Parameters:  
- nrounds (# Boosting Iterations)  
- max_depth (Max Tree Depth)  
- eta (Shrinkage)  
- gamma (Minimum Loss Reduction)  
- colsample_bytree (Subsample Ratio of Columns)   
- min_child_weight (Minimum Sum of Instance Weight)  

```{r warning=FALSE, message=FALSE, cache=TRUE}
# Set tuning parameter sets #
extrBoostGrid <- expand.grid(nrounds = (1:5)*100,
                             max_depth = c(2, 3),
                             eta = c(.01, .025, .05),
                             gamma = c(0),
                             colsample_bytree = 1,
                             min_child_weight = 1)

set.seed(seed)
priceXGBoostModel <- train(form = resultFormula, data = modelInput, 
                           method = 'xgbTree', trControl = customCvContr, 
                           tuneGrid = extrBoostGrid, metric = 'GainPerc',
                           objective = 'multi:softprob', num_class = 3, 
                           allowParallel = TRUE)

priceXGBoostModel$results[as.integer(rownames(
  priceXGBoostModel$results)) == as.integer(rownames(priceXGBoostModel$bestTune)), ]

trellis.par.set(caretTheme())
plot(priceXGBoostModel)
vioplot(priceXGBoostModel$resample$GainPerc, names = 'Profit [%]', col = 'green')
title('Violin Plot of Profit Percentage in resamples')
summary(priceXGBoostModel$resample$GainPerc)

# Non-resampled training performance
testPred <- predict(priceXGBoostModel, modelInput)
confMatrix <- confusionMatrix(testPred, reference = modelInput$matchResult)
confMatrix$overall[1:2]
```

TODO comparison with benchmarks


# Results and Forecast
TODO