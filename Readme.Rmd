---
title: "The Betlab Project"
author: "Tobias Diederich"
date: "Saturday, October 31, 2015"
output: pdf_document
---

## Abstract

The target of the Betlab project is to find the best model predicting the outcome (HomeVictory, Draw, VisitorsVictory) of football matches.
The best predicted probability simulates the highest percentage profit in relation to booky odds (Value Betting). The fundamental predictors are the aggregated marketprices of the participating players (parsed on transfermarkt.de).

I show the high potential of my approach, even if it is not yet practicable.

## Data

The match, team and player data are collected from [Transfermarkt](http://www.transfermarkt.de/). Booky odds are collected from [Sfstats](http://de.sfstats.net/). The parsers are written in Java and are not part of this paper.

Relevant data will be of germanies 1. Bundesliga from season 2005-2006 to 2014-2015. I included the english premier league too, but focus here for simplicity on BL1.

```{r warning=FALSE, message=FALSE, cache=TRUE}
source(file = 'production/loadData.R', echo = FALSE, encoding = 'UTF-8') 
toMatchday <- 34
seasons <- c('2005-2006', '2006-2007', '2007-2008', '2008-2009', '2009-2010',
             '2010-2011', '2011-2012', '2012-2013', '2013-2014', '2014-2015')
leagues <- c('BL1')
trainingRaw <- loadTrainingData(toMatchday = toMatchday, seasons = seasons, leagues = leagues)
matches <- trainingRaw$matches
odds <- trainingRaw$odds
stats <- trainingRaw$stats
```


### Datasets

matches -> contains all matches   
odds -> contains booky odds and probabilies for all matches  
stats -> an observation contains information for one player in one match  

Here is a brief exploration of the raw data:
```{r warning=FALSE, message=FALSE}
describe(dplyr:::select(matches, goalsHome, goalsVisitors, matchResult))
describe(dplyr:::select(odds, HomeVictory, VisitorsVictory, Draw))
describe(dplyr:::select(stats, fitPrice, position, playerAssignment, formation))
```


## Feature Engineering

The features I extract are the marketprices of participating players aggregated by team (Home, Visitors), grouped position (TW, DEF, MID, OFF) and aggregation method (min, max, avg, sum).
My first analysis is on including the players who played the whole match, who got substituted from bench and to bench. This is not practicable because I have an unrealistic information advantage in comparison to the booky. I stick to this approach at first, because I don't expect the advantage as big and I want to show the potential of this approach.

```{r warning=FALSE, message=FALSE, cache=TRUE}
source('./production/positionFeatureExtraction.R', 
       echo = FALSE, encoding = 'UTF-8')
### Preparation
#[1] "Torwart"               "Innenverteidiger"      "Linker Verteidiger"    "Rechter Verteidiger"   "Defensives Mittelfeld"
#[6] "Zentrales Mittelfeld"  "Linkes Mittelfeld"     "Rechtes Mittelfeld"    "Offensives Mittelfeld" "Haengende Spitze"     
#[11] "Mittelstuermer"        "Linksaussen"           "Rechtsaussen"
positions <- c('tw', 'def', 'def', 'def', 'mid', 'mid', 'mid', 'mid', 'off', 'off', 'off', 'off', 'off')
lineupAssignments <- c('DURCHGESPIELT', 'AUSGEWECHSELT', 'EINGEWECHSELT')
featuredMatches <- extractMatchResultFeatures(playerStats = stats,
                                            matches = matches,
                                            priceAssignedPositions = positions,
                                            functs = c('min', 'max', 'avg', 'sum'), 
                                            lineupAssignments)
# Select the relevant predictors
filteredFeatureMatches <- filterFeaturedMatches(featuredMatches)
```

Used Features:
```{r warning=FALSE, message=FALSE}
explMatches <- dplyr:::select(filteredFeatureMatches, -matchId, -matchResult, -goalsHome, - goalsVisitors)
colnames(explMatches)
```

```{r warning=FALSE, message=FALSE}
library(magrittr)
library(tidyr)
explGathered <- explMatches %>% gather(feature, value)

getGroupStr <- function(feature, group) {
    charList <- strsplit(as.character(feature), '_')
    charFrame <- data.frame(do.call(rbind, charList))
    if(group == 'func') {
        return(charFrame[, 4])
    } else if(group == 'pos') {
        return(charFrame[, 1])
    } else {
        return(NA)
    }
}
groupedMatches <- mutate(explGathered, funct = factor(getGroupStr(feature, 'func')),
                         position = factor(getGroupStr(feature, 'pos')))
avgPlot <- ggplot(filter(groupedMatches, funct == 'avg'), aes(x = position, y = value, fill = position)) +
    geom_boxplot() +
    ggtitle('Avg Prices of players by position') +
    coord_cartesian(ylim = c(0, 10000000))
avgPlot
```

No surprise here, offensive players are the most expensive.

## Model fitting and tuning

The target of the model tuning process is to find a model which maximizes the simulated profit [%]. This is a custom metric and implemented in the function betMetricsSummary. This function is integrated in the caret resampling and tuning process (caret is so great!!).

Model algorithms shown here are: POLR, Gradient boosting and extreme gradient boosting (tree). I tryed different models like: random forests, support vector machienes, C5.0, neural nets and knn. These method performances were bad in comparison, so they are not shown here.

**Remarks on custom metrics:**  
GainPerc is a custom metric, which calculates the simulated profit against Booky odds, if the model would have been applyed consistently. A bet is simulated, if the predicted probability of an outcome divided through the booky probability > 1.1.
ValueDiffPerc is a custom metric, which calculates the mean difference in percentage points of the predicted probability and the booky probability of the real outcome.

### Configuration of the fitting process

I use 5-fold cross validation first. 10-fold would be probably better, but calculation would take much longer. A static seed is set to make the results reproduceable.
```{r warning=FALSE, message=FALSE}
source(file = './production/models.R', 
       echo = FALSE, encoding = 'UTF-8')
seed <- 16450
customCvContr <- trainControl(method = 'cv', number = 5, classProbs = TRUE, 
                              summaryFunction = betMetricsSummary)

resultFormula <- as.formula('matchResult ~ . -matchId -goalsHome -goalsVisitors')
```

### POLR model

First I fit a linear POLR model for simplicity and because it regards the outcome as an ordered factor.

```{r warning=FALSE, message=FALSE, cache=TRUE}
set.seed(seed)
polrModel <- train(form = resultFormula, data = filteredFeatureMatches, method = 'polr',
                   preProcess = c('center', 'scale'), trControl = customCvContr)
polrModel
confusionMatrix(polrModel)

# POLR Resampling exploration
vioplot(polrModel$resample$GainPerc, names = 'Gain [%]', col = 'green')
title('Violin Plot of Gain Percentage in resamples')
summary(polrModel$resample$GainPerc)

# Training Performance without resampling
testPred <- predict(polrModel, filteredFeatureMatches)
confMatrix <- confusionMatrix(testPred, reference = filteredFeatureMatches$matchResult)
confMatrix$overall[1:2]
```
A profit of 21% is huge!! Model Accuracy and kappa are both much better than the booky metrics.

The gap between training and resampled training accuracy and kappa is small, 
thus I will use more complex models with lower bias.

TODO Explore correlations of metrics like 
GainPerc ~ I(Accuracy - BookyAccuracy) + I(Kappa - BookyKappa)
GainPerc ~ ValueDiffPerc

### Gradient Boosting

```{r warning=FALSE, message=FALSE, cache=TRUE}
gbmGrid <- expand.grid(.interaction.depth = c(1, 2, 3, 4),
                       .n.trees = seq(50, 1000, by = 200), 
                       .shrinkage = c(.05), 
                       .n.minobsinnode = c(10))
set.seed(seed)
gbmModel <- train(form = resultFormula, data = filteredFeatureMatches, method = 'gbm',
                  trControl = customCvContr, verbose = FALSE, 
                  tuneGrid = gbmGrid, distribution = 'multinomial',
                  metric = 'GainPerc')
# Best Tune
gbmModel$results[as.integer(rownames(gbmModel$results)) == as.integer(rownames(gbmModel$bestTune)), ]
# Plotting the resampling profile
trellis.par.set(caretTheme())
plot(gbmModel)
```

There is a slide increase in model performance (GainPerc and ValueDiffPerc). Here is space for some further fine tuning, but I encounter memory problems with gbm, so I try extreme gradient boosting.

### Extreme Gradiant Boosing

Tuning Parameters: 
- nrounds (# Boosting Iterations)
- max_depth (Max Tree Depth), 
- eta (Shrinkage)
- gamma (Minimum Loss Reduction)
- colsample_bytree (Subsample Ratio of Columns) 
- min_child_weight (Minimum Sum of Instance Weight)

```{r warning=FALSE, message=FALSE, cache=TRUE}
extrBoostGrid <- expand.grid(nrounds = (1:10)*100,
                             eta = c(.05, .075, .1),
                             max_depth = 1:4)
set.seed(seed)
extrBoostModel <- train(form = resultFormula, data = filteredFeatureMatches, method = 'xgbTree',
                        trControl = customCvContr, tuneGrid = extrBoostGrid, metric = 'GainPerc',
                        objective = 'multi:softprob', num_class = 3,
                        colsample_bytree = 1, min_child_weight = 1)
trellis.par.set(caretTheme())
plot(extrBoostModel)

extrBoostModel$results[as.integer(rownames(extrBoostModel$results)) == as.integer(rownames(extrBoostModel$bestTune)), ]

# Non-resampled training performance
testPred <- predict(extrBoostModel, filteredFeatureMatches)
confMatrix <- confusionMatrix(testPred, reference = filteredFeatureMatches$matchResult)
confMatrix$overall[1:2]
```


## Predictions with just the starting lineup

I don't trust the huge expected profit of ca. 25%. So, I reevaluate my predictions without an information advantage. This means I integrate just the players in the starting lineup (They are known before a match). Additionally I extract features for the players on bench.

```{r warning=FALSE, message=FALSE, cache=TRUE}
# Integrate starting lineup and additional features for players on bench
realLineupAssignments <- c('DURCHGESPIELT', 'AUSGEWECHSELT')
benchFuncts <- c('max', 'avg')
realFeaturedMatches <- extractMatchResultFeatures(playerStats = stats,
                                              matches = matches,
                                              priceAssignedPositions = positions,
                                              functs = c('min', 'max', 'avg', 'sum'), 
                                              realLineupAssignments,
                                              benchFuncts = benchFuncts)

realFilteredFeatureMatches <- filterFeaturedMatches(realFeaturedMatches)
```
Used Features:
```{r warning=FALSE, message=FALSE}
explMatches <- dplyr:::select(realFilteredFeatureMatches, -matchId, -matchResult, -goalsHome, - goalsVisitors)
colnames(explMatches)
```

### Tuning Models for a realistic and applicable approach

I tune POLR, GBM and extreme gradient boosting models.

```{r warning=FALSE, message=FALSE, cache=TRUE}
set.seed(seed)
polrModel <- train(form = resultFormula, data = realFilteredFeatureMatches, method = 'polr',
                   preProcess = c('center', 'scale'), trControl = customCvContr)
polrModel
```
Wow, what a huge difference. The expected small information advantage, is big in regard of the expected profit. Lets see if other models perform better.

### GBM 

```{r warning=FALSE, message=FALSE, cache=TRUE}
gbmGrid <- expand.grid(.interaction.depth = c(1, 2, 3, 4),
                       .n.trees = seq(50, 1000, by = 200), 
                       .shrinkage = c(.05), 
                       .n.minobsinnode = c(10))
set.seed(seed)
gbmModel <- train(form = resultFormula, data = realFilteredFeatureMatches, method = 'gbm',
                  trControl = customCvContr, verbose = FALSE, 
                  tuneGrid = gbmGrid, distribution = 'multinomial',
                  metric = 'GainPerc')
# Best Tune
gbmModel$results[as.integer(rownames(gbmModel$results)) == as.integer(rownames(gbmModel$bestTune)), ]
# Plotting the resampling profile
trellis.par.set(caretTheme())
plot(gbmModel)
```

### Extreme Boosting
```{r warning=FALSE, message=FALSE, cache=TRUE}
extrBoostGrid <- expand.grid(nrounds = (1:10)*100,
                             eta = c(.05, .075, .1),
                             max_depth = 1:4)
set.seed(seed)
extrBoostModel <- train(form = resultFormula, data = realFilteredFeatureMatches, method = 'xgbTree',
                        trControl = customCvContr, tuneGrid = extrBoostGrid, metric = 'GainPerc',
                        objective = 'multi:softprob', num_class = 3,
                        colsample_bytree = 1, min_child_weight = 1)
extrBoostModel$results[as.integer(rownames(extrBoostModel$results)) == as.integer(rownames(extrBoostModel$bestTune)), ]
trellis.par.set(caretTheme())
plot(extrBoostModel)
vioplot(extrBoostModel$resample$GainPerc, names = 'Gain [%]', col = 'green')
title('Violin Plot of Gain Percentage in resamples')
summary(extrBoostModel$resample$GainPerc)
```

# Results and Forecast

The results are not overwhelming. The final model is very unstable concerning the profit (high SD). The minimum requirement for applying the model on real bets is a positive GainPerc over all folds and at least > 5%.
To achieve this, I think it is necessary to integrate new features. Imaginable features could describe team formations or distance of the visiting team to travel. A disadvantage of featured based on marketprices of players is, that the marketprices are created twice a year, so they describe the long term quality of a player. A way to integrate short term form predictors of players is to engineer features based on the kicker.de grade associated to players in past matches.
